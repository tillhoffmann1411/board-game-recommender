# ETL Pipeline

Data extraction, transformation, and loading pipeline for the Board Game Recommender.

## Overview

The ETL pipeline:
1. **Extracts** data from Board Game Geek (BGG) and online game platforms
2. **Transforms** raw data into MongoDB-compatible documents
3. **Loads** data into MongoDB with proper indexes

## Quick Start

### Prerequisites
- Python 3.11+
- MongoDB (local or Atlas)
- Docker (optional)

### Using Docker (Recommended)

```bash
# Start MongoDB
docker compose up -d mongodb

# Run ETL pipeline
docker compose --profile etl up etl

# Optional: Start MongoDB admin UI
docker compose --profile admin up -d mongo-express
# Access at http://localhost:8081
```

### Using Python Directly

```bash
# Install dependencies
cd etl
pip install -r requirements.txt

# Set environment variables
export MONGODB_URI="mongodb://localhost:27017"
export MONGODB_DB="board-game-recommender"

# Run pipeline
python -m etl.pipeline --data-dir ../data
```

## Configuration

All configuration is via environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `MONGODB_URI` | MongoDB connection string | Required |
| `MONGODB_DB` | Database name | `board-game-recommender` |
| `ETL_DATA_DIR` | Data directory path | `./data` |
| `ETL_RUN_BGG` | Run BGG extraction (scraping) | `false` |
| `ETL_BGG_MAX_PAGES` | Maximum pages to scrape from BGG | `10` |
| `ETL_RUN_ONLINE` | Run online games extraction | `false` |
| `ETL_RUN_TRANSFORM` | Run transformation | `true` |
| `ETL_RUN_LOAD` | Run loading to MongoDB | `true` |
| `ETL_MIN_REVIEWS_GAME` | Min reviews per game | `500` |
| `ETL_MIN_REVIEWS_USER` | Min reviews per user | `5` |
| `ETL_BATCH_SIZE` | Batch size for loading | `1000` |
| `ETL_MAX_RETRIES` | Max retry attempts | `3` |
| `ETL_RETRY_DELAY` | Initial retry delay (seconds) | `1.0` |

## Data Directory Structure

The pipeline expects data in this structure:

```
data/
├── raw/                    # Raw extracted data
│   └── bgg_games.csv       # Scraped BGG games (generated by extraction)
├── Joined/
│   └── Results/
│       ├── BoardGames.csv
│       ├── User.csv
│       ├── Reviews_Reduced.csv
│       ├── Categories.csv
│       ├── Category_Game_Relation.csv
│       ├── Mechanics.csv
│       ├── Mechanic_Game_Relation.csv
│       ├── Designer.csv
│       ├── Designer_Game_Relation.csv
│       ├── Publisher.csv
│       └── Publisher_Game_Relation.csv
├── Onlinegames/
│   └── Processed/
│       └── online_games.csv
└── Recommender/
    └── item-item-sim-matrix-surprise-Reduced_dataset-LONG_FORMAT.csv
```

## Module Structure

```
etl/
├── config.py       # Configuration management
├── logger.py       # Logging setup
├── utils.py        # Utilities (retry, helpers)
├── transform.py    # Data transformers
├── load.py         # MongoDB loader
├── pipeline.py     # Main orchestrator
├── migrate.py      # Legacy data migration
├── extraction/     # Web scraping modules
│   ├── __init__.py
│   └── bgg_scraper.py  # BoardGameGeek scraper
├── lib/
│   └── mongodb.py  # MongoDB helper class
├── Dockerfile      # Container image
├── requirements.txt
└── README.md
```

## CLI Usage

```bash
# Run with default settings
python -m etl.pipeline

# Specify data directory
python -m etl.pipeline --data-dir /path/to/data

# Verbose logging
python -m etl.pipeline --log-level DEBUG

# Custom log directory
python -m etl.pipeline --log-dir /var/log/etl
```

## BGG Scraping

The pipeline includes a Selenium-based scraper for BoardGameGeek.com that extracts game information from the browse pages.

### Running Scraping

```bash
# Enable BGG extraction in pipeline
export ETL_RUN_BGG=true
export ETL_BGG_MAX_PAGES=10  # Limit pages (default: 10)
python -m etl.pipeline

# Or run scraper standalone
python -m etl.extraction.bgg_scraper --max-pages 10 --output data/bgg_games.csv
```

### Scraped Data

The scraper extracts:
- Game name and detail page URL
- BGG ID
- Rank, ratings (Geek Rating, Average Rating)
- Number of voters
- Year published
- Thumbnail image URL
- Description

Output is saved as CSV to `data/raw/bgg_games.csv` (or specified path).

### Docker Requirements

The ETL Docker image includes Chrome/Chromium for Selenium. When running in Docker, the scraper runs in headless mode automatically.

## Migrating from Legacy CSV

If you have existing CSV data from the old PostgreSQL-based ETL:

```bash
python -m etl.migrate --data-dir ../Data
```

## Retry Handling

The pipeline includes automatic retry with exponential backoff for:
- API requests
- Database operations

Configure via `ETL_MAX_RETRIES` and `ETL_RETRY_DELAY`.

## Logging

Logs are written to:
- Console (stdout)
- File (`logs/etl_YYYYMMDD_HHMMSS.log`)

Log levels: DEBUG, INFO, WARNING, ERROR

## Data Transformations

### Games
- Categories, mechanics, designers, publishers are denormalized (embedded)
- Legacy integer IDs mapped to MongoDB ObjectIds
- BGG ratings stored as nested objects

### Users
- Imported users get synthetic `clerkId` (e.g., `imported_bgg_12345`)
- Rating counts denormalized for quick access

### Ratings
- Separate collection for query efficiency
- Origin tracking (app, bgg)

### Similarities
- Precomputed item-item similarity matrix
- Top 100 similar games per game
- Stored as nested array in single document per game

## Troubleshooting

### "Data directory not found"
Ensure the data directory exists and contains the expected CSV files.

### "MONGODB_URI environment variable is required"
Set the MongoDB connection string:
```bash
export MONGODB_URI="mongodb://localhost:27017"
```

### Connection timeout
Check that MongoDB is running and accessible at the configured URI.

## Development

```bash
# Install dev dependencies
pip install -r requirements.txt

# Run tests (when available)
pytest

# Type checking
mypy etl/
```
